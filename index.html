<!Doctype html>
<html lang="en">
    <head>
        <title>Despoina Paschalidou</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div class="header">
            <div class="section">
                <div class="myname">Despoina Paschalidou</div>
                <div class="menu">
                    <ul>
                        <li>
                            <a href="https://paschalidoud.github.io">Home</a>
                        </li>
                        <li>
                            <a href="https://paschalidoud.github.io/publications">Publications</a>
                        </li>
                        <li>
                            <a href="https://paschalidoud.github.io/talks">Presentations</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="section">
            <div class="row">
                <div class="image" id="profile-photo">
                    <a href=https://www.google.com/search?q=%CE%BA%CE%BF%CF%85%CF%86%CE%BF%CE%BD%CE%AE%CF%83%CE%B9%CE%B1&tbm=isch&ved=2ahUKEwj7jrmr-4rpAhVEt6QKHdQLBBsQ2-cCegQIABAA&oq=%CE%BA%CE%BF%CF%85%CF%86%CE%BF%CE%BD%CE%AE%CF%83%CE%B9%CE%B1&gs_lcp=CgNpbWcQA1AAWABgzL0BaABwAHgAgAEAiAEAkgEAmAEAqgELZ3dzLXdpei1pbWc&sclient=img&ei=SAyoXvvKIsTukgXUl5DYAQ&bih=949&biw=1920>
                    <img alt="Profile photo" src="figures/me-small.jpg" />
                    </a>
                </div>
                <div class="content">
                    <p>
                        I am Postdoctoral Researcher at <a href=https://www.stanford.edu/>Stanford University</a>
                        working with <a href=https://geometry.stanford.edu/member/guibas/index.html>Prof. Leonidas Guibas</a> at the
                        <a href=https://geometry.stanford.edu/index.html>
                        Geometric Computation Group</a>.
                        I did my PhD at the
                        <a href=https://learning-systems.org/>
                        Max Planck ETH Center for Learning Systems </a>, where I worked with
                        <a href=http://www.cvlibs.net/>Prof. Andreas Geiger</a>
                        and <a href=https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1>
                        Prof. Luc van Gool</a>.
                        Prior to this, I was an undergraduate in the School of Electrical and
                        Computer Engineering in the
                        <a href=https://www.auth.gr/en/ee>Aristotle University of Thessaloniki</a>
                        in Greece, where I worked with 
                        <a href=https://mug.ee.auth.gr/people/anastasios-delopoulos/>
                        Prof. Anastasios Delopoulos</a> and
                        <a href=https://diou.github.io/>
                        Prof. Christos Diou</a>. During my PhD, I was very lucky to
                        have spent one wonderful year working with <a href="https://www.cs.utoronto.ca/~fidler/">
                        Prof. Sanja Fidler</a> at NVIDIA Research, where I worked on 3D Generative AI.
                        Moreover, I also had the pleasure to intern at FAIR
                        under the guidance of <a href="https://d-novotny.github.io/">Dr. David Novotny</a> and
                        <a href="https://www.robots.ox.ac.uk/~vedaldi/">Prof. Andrea Vedaldi</a>, where I worked
                        towards developing 3D-aware representations from videos.
                        I am interested in Computer Vision, particularly in the
                        areas of interpretable shape representations, scene
                        understanding and 3D controllable generative models
                        for objects and scenes.
                    </p>
                    <p style="text-align:center">
                        <span class="icon">
                            <a href=mailto:paschald@stanford.edu>
                                <i class="fa fa-envelope fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://scholar.google.de/citations?user=zxFlR6sAAAAJ&hl=en&oi=ao>
                            <i class="ai ai-google-scholar ai-2x"></i>
                            </a> 
                        </span>
                        <span class="icon">
                            <a href=https://github.com/paschalidoud>
                               <i class="fa fa-github fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://twitter.com/paschalidoud_1>
                                <i class="fa fa-twitter fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://www.research-collection.ethz.ch/handle/20.500.11850/521013>
                                <i class="fa fa-solid fa-graduation-cap fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                    </p>
                </div>
            </div>
            <div class="title">
                <span>News</span>
            </div>
            <ul class=news_item>
                <li><span class="news-date">Feb 2024:</span><span class="news-text">Three papers accepted to CVPR 2024! <a href="http://raywzy.com/CAD/">CAD</a>, <a href="https://arxiv.org/pdf/2303.12050.pdf">CurvecloudNet</a> and <a href="">MultiPhys</a>!
                <li><span class="news-date">Jan 2024:</span><span class="news-text">I am an Area Chair for <a href="https://eccv2024.ecva.net/">ECCV 2024!</a>
                <li><span class="news-date">Dec 2023:</span><span class="news-text"> We are organizing the <a href="https://ai3dg.github.io/">1st Workshop on AI for 3D Generation</a> and the
 <a href="https://opensun3d.github.io/">2nd Workshop on Open-Vocabulary 3D Scene Understanding</a>
                at CVPR@2024 in Seattle!!</span></li>
                <li><span class="news-date">Nov 2023:</span><span class="news-text">I am very honored to be one of the <a href="https://eecsrisingstars2023.cc.gatech.edu/participants/Despoina_Paschalidou/">EECS Rising Stars 2023!</a></span></li>
                <li><span class="news-date">Jul 2023:</span><span class="news-text"> Two papers accepted to ICCV 2023! <a href="https://sites.google.com/stanford.edu/copilot">COPILOT</a>
                and <a href="https://sherwinbahmani.github.io/cc3d/">CC3D</a>!</span></li>
                <li><span class="news-date">Jun 2023:</span><span class="news-text"> I am an Area Chair for <a href="https://3dvconf.github.io/2024/">3DV 2024</a> and <a href="https://wacv2024.thecvf.com/">WACV 2024</a>!</span></a>
                <li><span class="news-date">Jun 2023:</span><span class="news-text"> We are organizing the <a href="https://ai3dcc.github.io/">1st Workshop on AI for 3D Content Creation</a>
                at ICCV@2023 in Paris!!</span></li>
                <li><span class="news-date">May 2023:</span><span class="news-text"> One paper accepted to TMLR 2023!</span></li>
                <li><span class="news-date">May 2023:</span><span class="news-text"> Received Outstanding Reviewer Award at <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers">CVPR 2023</a>!</span></li>
                <li><span class="news-date">Feb 2023:</span><span class="news-text"> Two papers accepted to CVPR 2023!
                <a href="https://ktertikas.github.io/part_nerf">PartNeRF</a> and <a href="https://visual.ee.ucla.edu/alto.htm/">ALTO</a>!
                </span></li>
                <li><span class="news-date">Feb 2023:</span><span class="news-text"> We are organizing the <a href="https://struco3d.github.io/cvpr2023/">Second Workshop on Structural and Compositional Learning on 3D Data</a> at CVPR@2023 in Vancouver!!</span></li>
                <li><span class="news-date">May 2022:</span><span class="news-text"> Received Outstanding Reviewer Award at <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR 2022</a>!</span></li>
                <!--<li><span class="news-date">Feb 2022:</span><span
                class="news-text"> I started as a Postdoctoral Researcher at Stanford with <a href=https://geometry.stanford.edu/member/guibas/index.html>Prof. Leonidas Guibas</a>!
                </span></li>-->
                <!---<li><span class="news-date">December 21, 2021:</span><span
                class="news-text"> After 4 wonderful years I successfully defended my PhD!</span></li>--!>
                <!--<li><span class="news-date">Sep 2021: </span><span class="news-text"> Our paper <a href=https://nv-tlabs.github.io/ATISS/>ATISS</a> got accepted in NeurIPS 2021!!
                </span></li>-->
                <!---<li><span class="news-date">June 21, 2021:</span><span class="news-text"> I am interning at <a href="https://ai.facebook.com/">FAIR</a> in London this summer!!</span></li>--!>
                <!--<li><span class="news-date">Mar 2021: </span><span class="news-text"> Our paper <a href=https://paschalidoud.github.io/neural_parts>Neural Parts</a>
                got accepted in CVPR 2021!!-->
                <!-- <a href=https://github.com/paschalidoud/neural_parts>Code</a> and pre-trained models are available!
                Also check out our learned primitives using our cool <a href=https://paschalidoud.github.io/neural_parts>interactive demo</a>.-->
                </span></li>
                <!--<li><span class="news-date">May 2021:</span><span class="news-text"> Received Outstanding Reviewer Award at <a href="https://cvpr2021.thecvf.com/node/184">CVPR 2021</a>!</span></li>-->
                <!--<li><span class="news-date">Mar  2020:</span><span class="news-text"> We released <a href="http://simple-3dviz.com">simple-3dviz</a>
                a library for 3D visualization using Python and OpenGL.-->
                <!--<a href=https://github.com/angeloskath/simple-3dviz>Code</a> and
                <a href="http://simple-3dviz.com/">documentation</a> are available!</span></li>-->
                <!---<li><span class="news-date">February 23, 2020:</span><span class="news-text"> Our paper on <a href="https://github.com/paschalidoud/hierarchical_primitives">
                unsupervised hierarchical primitive-based reconstruction</a> was accepted in CVPR 2020.</span></li>
                <li><span class="news-date">February 10, 2020:</span><span class="news-text"> I am interning at <a href="https://www.nvidia.com/en-us/">NVIDIA Research</a> in Toronto this summer!!</span></li>
                <li><span class="news-date">May 05, 2019:</span><span class="news-text"> We released the PyTorch code for our 
                <a href=https://github.com/paschalidoud/superquadric_parsing>Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids</a> paper.</span></li>
                <li><span class="news-date">March 02, 2019:</span><span class="news-text"> Two papers accepted in CVPR 2019.</span></li>
                <li><span class="news-date">February 28, 2018:</span><span class="news-text"> <a href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.pdf>RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials</a> was accepted in CVPR 2018. Check our <a href="http://raynet-mvs.com/">project page</a> for code and documentation.</span>
                </li>--!>
            </ul>
            <div class="title">
                <span>Selected Publications</span>
            </div>
            <!-- start publication list --><div class="row paper"><div class="image"><img src="teasers/cc3d_teaser_2.png" alt="CC3D: Layout-Conditioned Generation of Compositional 3D Scenes" /></div><div class="content"><div class="paper-title"><a href="https://sherwinbahmani.github.io/cc3d/">CC3D: Layout-Conditioned Generation of Compositional 3D Scenes</a></div><div class="conference">International Conference on Computer Vision (ICCV), 2023</div><div class="authors"><a href="https://sherwinbahmani.github.io/" class="author">Sherwin Bahmani</a>, <a href="" class="author">Jeong Joon Park</a>, <strong class="author">Despoina Paschalidou</strong>, <a href="http://yanxg.art/" class="author">Xingguang Yan</a>, <a href="https://stanford.edu/~gordonwz/" class="author">Gordon Wetzstein</a>, <a href="https://geometry.stanford.edu/member/guibas/" class="author">Leonidas Guibas</a>, <a href="https://taiya.github.io/" class="author">Andrea Tagliasacchi</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://sherwinbahmani.github.io/cc3d/" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2303.12074.pdf" data-type="Paper">Paper</a> <a href="data/Bahmani2023ICCV_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/sherwinbahmani/cc3d" data-type="Code">Code</a> <a href="#" data-type="Bibtex" data-index="5">Bibtex</a><div class="link-content" data-index="0">In this work, we introduce CC3D, a conditional generative model that synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained using single-view images. Different from most existing 3D GANs that limit their applicability to aligned single objects, we focus on generating complex scenes with multiple objects, by modeling the compositional nature of 3D scenes. By devising a 2D layoutbased approach for 3D synthesis and implementing a new 3D field representation with a stronger geometric inductive bias, we have created a 3D GAN that is both efficient and of high quality, while allowing for a more controllable generation process. Our evaluations on synthetic 3D-FRONT and real-world KITTI-360 datasets demonstrate that our model generates scenes of improved visual and geometric quality in comparison to previous works.</div><div class="link-content" data-index="5"><pre>@InProceedings{Bahmani2023ICCV,
  author = {Bahmani, Sherwin and Park, Jeong Joon and Paschalidou, Despoina and Yan, Xingguang and Wetzstein, Gordon and Guibas, Leonidas and Tagliasacchi, Andrea},
  title = {CC3D: Layout-Conditioned Generation of Compositional 3D Scenes},
  booktitle = {International Conference on Computer Vision (ICCV)}},
  year = {2023}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/partnerf_2.png" alt="PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision" /></div><div class="content"><div class="paper-title"><a href="https://ktertikas.github.io/part_nerf">PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2023</div><div class="authors"><a href="https://ktertikas.github.io/" class="author">Konstantinos Tertikas</a>, <strong class="author">Despoina Paschalidou</strong>, <a href="https://cs.stanford.edu/~bxpan/" class="author">Boxiao Pan</a>, <a href="" class="author">Jeong Joon Park</a>, <a href="" class="author">Mikaela Angelina Uy</a>, <a href="https://cgi.di.uoa.gr/~emiris/index-eng.html" class="author">Ioannis Emiris</a>, <a href="https://avrithis.net/" class="author">Yannis Avrithis</a>, <a href="https://geometry.stanford.edu/member/guibas/" class="author">Leonidas Guibas</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://ktertikas.github.io/part_nerf" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2303.09554.pdf" data-type="Paper">Paper</a> <a href="data/Tertikas2023CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="slides/Tertikas2023CVPR_slides.pdf" data-type="Slides">Slides</a> <a href="https://github.com/ktertikas/part_nerf" data-type="Code">Code</a> <a href="#" data-type="Bibtex" data-index="6">Bibtex</a><div class="link-content" data-index="0">Impressive progress in generative models and implicit representations gave rise to methods that can generate 3D shapes of high quality. However, being able to locally control and edit shapes is another essential property that can unlock several content creation applications. Local control can be achieved with part-aware models, but existing methods require 3D supervision and cannot produce textures. In this work, we devise PartNeRF, a novel part-aware generative model for editable 3D shape synthesis that does not require any explicit 3D supervision. Our model generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables several editing operations such as applying transformations on parts, mixing parts from different objects etc. To ensure distinct, manipulable parts we enforce a hard assignment of rays to parts that makes sure that the color of each ray is only determined by a single NeRF. As a result, altering one part does not affect the appearance of the others. Evaluations on various ShapeNet categories demonstrate the ability of our model to generate editable 3D objects of improved fidelity, compared to previous part-based generative approaches that require 3D supervision or models relying on NeRFs.</div><div class="link-content" data-index="6"><pre>@InProceedings{Tertikas2023CVPR,
  author    = {Konstantinos Tertikas and Despoina Paschalidou and Boxiao Pan and Jeong Joon Park and Mikaela Angelina Uy and Ioannis Emiris and Yannis Avrithis and Leonidas Guibas},
  title     = {PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/alto_teaser.png" alt="ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction" /></div><div class="content"><div class="paper-title"><a href="https://visual.ee.ucla.edu/alto.htm/">ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2023</div><div class="authors"><a href="https://zhenwangwz.github.io/" class="author">Zhen Wang</a>, <a href="https://www.linkedin.com/in/shijie-zhou-ucla" class="author">Shijie Zhou</a>, <a href="" class="author">Jeong Joon Park</a>, <strong class="author">Despoina Paschalidou</strong>, <a href="" class="author">Suya You</a>, <a href="https://stanford.edu/~gordonwz/" class="author">Gordon Wetzstein</a>, <a href="https://geometry.stanford.edu/member/guibas/" class="author">Leonidas Guibas</a>, <a href="https://www.ee.ucla.edu/achuta-kadambi/" class="author">Achuta Kadambi</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://visual.ee.ucla.edu/alto.htm/" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2212.04096.pdf" data-type="Paper">Paper</a> <a href="data/Zhen2023CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="slides/presentation_alto.pdf" data-type="Slides">Slides</a> <a href="https://github.com/wzhen1/ALTO" data-type="Code">Code</a> <a href="https://www.youtube.com/watch?v=EsnE4dEIArY" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="7">Bibtex</a><div class="link-content" data-index="0">This work introduces alternating latent topologies (ALTO) for high-fidelity reconstruction of implicit 3D surfaces from noisy point clouds. Previous work identifies that the spatial arrangement of latent encodings is important to recover detail. One school of thought is to encode a latent vector for each point (point latents). Another school of thought is to project point latents into a grid (grid latents) which could be a voxel grid or triplane grid. Each school of thought has tradeoffs. Grid latents are coarse and lose high-frequency detail. In contrast, point latents preserve detail. However, point latents are more difficult to decode into a surface, and quality and runtime suffer. In this paper, we propose ALTO to sequentially alternate between geometric representations, before converging to an easy-to-decode latent. We find that this preserves spatial expressiveness and makes decoding lightweight. We validate ALTO on implicit 3D recovery and observe not only a performance improvement over the state-of-the-art, but a runtime improvement of 3-10Ã—.</div><div class="link-content" data-index="7"><pre>@InProceedings{Zhen2023CVPR,
    title = {ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction},
    author = {Wang, Zhen and Zhou, Shijie and Park, Jeong Joon and Paschalidou, Despoina and You, Suya and Wetzstein, Gordon and Guibas, Leonidas and Kadambi, Achuta},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/atiss.png" alt="ATISS: Autoregressive Transformers for Indoor Scene Synthesis" /></div><div class="content"><div class="paper-title"><a href="https://nv-tlabs.github.io/ATISS/">ATISS: Autoregressive Transformers for Indoor Scene Synthesis</a></div><div class="conference">Advances in Neural Information Processing Systems (NeurIPS), 2021</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://amlankar.github.io/" class="author">Amlan Kar</a>, <a href="http://shumash.com/" class="author">Maria Shugrina</a>, <a href="https://scholar.google.de/citations?user=rFd-DiAAAAAJ&hl=de" class="author">Karsten Kreis</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://www.cs.utoronto.ca/~fidler/" class="author">Sanja Fidler</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://nv-tlabs.github.io/ATISS/#" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2110.03675.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2021NEURIPS_poster.pdf" data-type="Poster">Poster</a> <a href="data/Paschalidou2021NEURIPS_slides.pdf" data-type="Slides">Slides</a> <a href="https://github.com/nv-tlabs/atiss" data-type="Code">Code</a> <a href="https://www.youtube.com/watch?v=VNY0BFMi2j4" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="7">Bibtex</a><div class="link-content" data-index="0">The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8x faster than existing methods.</div><div class="link-content" data-index="7"><pre>@InProceedings{Paschalidou2021NEURIPS,
  author = {Despoina Paschalidou and Amlan Kar and Maria Shugrina and Karsten Kreis and Andreas Geiger and Sanja Fidler},
  title = {ATISS: Autoregressive Transformers for Indoor Scene Synthesis},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/neural_parts.png" alt="Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks" /></div><div class="content"><div class="paper-title"><a href="https://paschalidoud.github.io/neural_parts">Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2021</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://angeloskath.github.io/" class="author">Angelos Katharopoulos</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://www.cs.utoronto.ca/~fidler/" class="author">Sanja Fidler</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://paschalidoud.github.io/neural_parts" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2103.10429.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2021CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/neural_parts" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/neural-parts/" data-type="Blog">Blog</a> <a href="http://www.cvlibs.net/publications/Paschalidou2021CVPR_slides.pdf" data-type="Slides">Slides</a> <a href="https://www.youtube.com/watch?v=6WK3B0IZJsw" data-type="Video">Video</a> <a href="https://www.itzikbs.com/neural-parts-learning-expressive-3d-shape-abstractions-with-invertible-neural-networks" data-type="Podcast">Podcast</a> <a href="#" data-type="Bibtex" data-index="9">Bibtex</a><div class="link-content" data-index="0">Impressive progress in 3D shape extraction led to representations that can capture object geometries with high fidelity. In parallel, primitive-based methods seek to represent objects as semantically consistent part arrangements. However, due to the simplicity of existing primitive representations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We address the trade-off between reconstruction quality and number of parts with Neural Parts, a novel 3D primitive representation that defines primitives using an Invertible Neural Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efficient computation of both the implicit surface function of a primitive and its mesh, without any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrangements without any part-level supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods.</div><div class="link-content" data-index="9"><pre>@InProceedings{Paschalidou2021CVPR,
    title = {Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks},
    author = {Paschalidou, Despoina and Katharopoulos, Angelos and Geiger, Andreas and Fidler, Sanja},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2021}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/hierarchical_primitives.png" alt="Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image" /></div><div class="content"><div class="paper-title"><a href="http:superquadrics.com/hierarchical_primitives">Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2020</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" class="author">Luc van Gool</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="http:superquadrics.com/hierarchical_primitives" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2004.01176.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2020CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/hierarchical_primitives" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/hierarchical-primitives/" data-type="Blog">Blog</a> <a href="http://www.cvlibs.net/publications/Paschalidou2020CVPR_slides.pdf" data-type="Slides">Slides</a> <a href="https://www.youtube.com/watch?v=QgD0NHbWVlU&vq=hd1080&autoplay=1" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="8">Bibtex</a><div class="link-content" data-index="0">Humans perceive the 3D world as a set of distinct objects that are characterized by various low-level (geometry, reflectance) and high-level (connectivity, adjacency, symmetry) properties. Recent methods based on convolutional neural networks (CNNs) demonstrated impressive progress in 3D reconstruction, even when using a single 2D image as input. However, the majority of these methods focuses on recovering the local 3D geometry of an object without considering its part-based decomposition or relations between parts. We address this challenging problem by proposing a novel formulation that allows to jointly recover the geometry of a 3D object as a set of primitives as well as their latent hierarchical structure without part-level supervision. Our model recovers the higher level structural decomposition of various objects in the form of a binary tree of primitives, where simple parts are represented with fewer primitives and more complex parts are modeled with more components. Our experiments on the ShapeNet and D-FAUST datasets demonstrate that considering the organization of parts indeed facilitates reasoning about 3D geometry.</div><div class="link-content" data-index="8"><pre>@InProceedings{Paschalidou2020CVPR,
    title = {Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image},
    author = {Paschalidou, Despoina and Luc van Gool and Geiger, Andreas},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2020},
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/superquadrics_revisited.png" alt="Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids" /></div><div class="content"><div class="paper-title"><a href="http:superquadrics.com">Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2019</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://scholar.google.de/citations?user=fkqdDEEAAAAJ&hl=en" class="author">Ali Osman Ulusoy</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="http:superquadrics.com/learnable-superquadrics.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/1904.09970.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2019CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/superquadric_parsing" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/superquadrics-revisited/" data-type="Blog">Blog</a> <a href="https://www.youtube.com/watch?v=eaZHYOsv9Lw" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="7">Bibtex</a><div class="link-content" data-index="0">Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.</div><div class="link-content" data-index="7"><pre>@InProceedings{Paschalidou2019CVPR,
    title = {Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids},
    author = {Paschalidou, Despoina and Ulusoy, Ali Osman and Geiger, Andreas},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2019},
}</pre></div></div></div></div><!-- end publication list -->

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143288088-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-143288088-1');
        </script>
    </body>
</html>
