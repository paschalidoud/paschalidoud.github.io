<!Doctype html>
<html lang="en">
    <head>
        <title>Despoina Paschalidou</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988897748">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div class="header">
            <div class="section">
                <div class="myname">Despoina Paschalidou</div>
                <div class="menu">
                    <ul>
                        <li>
                            <a href="https://paschalidoud.github.io">Home</a>
                        </li>
                        <li>
                            <a href="https://paschalidoud.github.io/publications">Publications</a>
                        </li>
                        <li>
                            <a href="https://paschalidoud.github.io/talks">Presentations</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="section">
            <div class="row">
                <div class="image" id="profile-photo">
                    <a href=https://www.google.com/search?q=%CE%BA%CE%BF%CF%85%CF%86%CE%BF%CE%BD%CE%AE%CF%83%CE%B9%CE%B1&tbm=isch&ved=2ahUKEwj7jrmr-4rpAhVEt6QKHdQLBBsQ2-cCegQIABAA&oq=%CE%BA%CE%BF%CF%85%CF%86%CE%BF%CE%BD%CE%AE%CF%83%CE%B9%CE%B1&gs_lcp=CgNpbWcQA1AAWABgzL0BaABwAHgAgAEAiAEAkgEAmAEAqgELZ3dzLXdpei1pbWc&sclient=img&ei=SAyoXvvKIsTukgXUl5DYAQ&bih=949&biw=1920>
                    <img alt="Profile photo" src="figures/me-small.jpg" />
                    </a>
                </div>
                <div class="content">
                    <p>
                        I am a PhD student at the
                        <a href=https://learning-systems.org/>
                        Max Planck ETH Center for Learning Systems </a>,
                        advised by  <a href=http://www.cvlibs.net/> Andreas Geiger</a>
                        and <a href=https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1>
                        Luc van Gool</a>.
                        Prior to this, I was an undergraduate in the School of Electrical and
                        Computer Engineering in the
                        <a href=https://www.auth.gr/en/ee>Aristotle University of Thessaloniki</a>
                        in Greece, where I worked with 
                        <a href=https://mug.ee.auth.gr/people/anastasios-delopoulos/>
                        Anastasios Delopoulos</a> and
                        <a href=https://mug.ee.auth.gr/people/christos-diou/>
                        Christos Diou</a>.
                        My research is in computer vision, particularly in the
                        areas of interpretable shape representations, scene understanding and
                        unsupervised deep learning.
                    </p>
                    <p style="text-align:center">
                        <span class="icon">
                            <a href=mailto:despoina.paschalidou@tue.mpg.de>
                                <i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://scholar.google.de/citations?user=zxFlR6sAAAAJ&hl=en&oi=ao>
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                            </a> 
                        </span>
                        <span class="icon">
                            <a href=https://github.com/paschalidoud>
                               <i class="fa fa-github-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://www.linkedin.com/in/paschalidoud>
                                <i class="fa fa-linkedin-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://twitter.com/paschalidoud_1>
                                <i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                    </p>
                </div>
            </div>
            <div class="title">
                <span>News</span>
            </div>
            <ul class=news_item>
                <li><span class="news-date">June 21, 2021:</span><span class="news-text"> I am interning at <a href="https://ai.facebook.com/">FAIR</a> in London this summer!!</span></li>
                <li><span class="news-date">March 18, 2021: </span><span class="news-text"> Our paper <a href=https://paschalidoud.github.io/neural_parts>Neural Parts</a>
                got accepted in CVPR 2021!! <a href=https://github.com/paschalidoud/neural_parts>Code</a> and pre-trained models are available!
                Also check out our learned primitives using our cool <a href=https://paschalidoud.github.io/neural_parts>interactive demo</a>. </span></li>
                <li><span class="news-date">March 31, 2020:</span><span class="news-text"> We released <a href="http://simple-3dviz.com">simple-3dviz</a>
                a library for 3D visualization using Python and OpenGL. <a href=https://github.com/angeloskath/simple-3dviz>Code</a> and
                <a href="http://simple-3dviz.com/">documentation</a> are available!</span></li>
                <li><span class="news-date">February 23, 2020:</span><span class="news-text"> Our paper on <a href="https://github.com/paschalidoud/hierarchical_primitives">
                unsupervised hierarchical primitive-based reconstruction</a> was accepted in CVPR 2020.</span></li>
                <li><span class="news-date">February 10, 2020:</span><span class="news-text"> I am interning at <a href="https://www.nvidia.com/en-us/">NVIDIA Research</a> in Toronto this summer!!</span></li>
                <li><span class="news-date">May 05, 2019:</span><span class="news-text"> We released the PyTorch code for our 
                <a href=https://github.com/paschalidoud/superquadric_parsing>Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids</a> paper.</span></li>
                <li><span class="news-date">March 02, 2019:</span><span class="news-text"> Two papers accepted in CVPR 2019.</span></li>
                <li><span class="news-date">February 28, 2018:</span><span class="news-text"> <a href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.pdf>RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials</a> was accepted in CVPR 2018. Check our <a href="http://raynet-mvs.com/">project page</a> for code and documentation.</span></li>
            </ul>
            <div class="title">
                <span>Selected Publications</span>
            </div>
            <!-- start publication list --><div class="row paper"><div class="image"><img src="teasers/atiss.png" alt="ATISS: Autoregressive Transformers for Indoor Scene Synthesis" /></div><div class="content"><div class="paper-title"><a href="https://nv-tlabs.github.io/ATISS/">ATISS: Autoregressive Transformers for Indoor Scene Synthesis</a></div><div class="conference">Advances in Neural Information Processing Systems (NeurIPS), 2021</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://amlankar.github.io/" class="author">Amlan Kar</a>, <a href="http://shumash.com/" class="author">Maria Shugrina</a>, <a href="https://scholar.google.de/citations?user=rFd-DiAAAAAJ&hl=de" class="author">Karsten Kreis</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://www.cs.utoronto.ca/~fidler/" class="author">Sanja Fidler</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://nv-tlabs.github.io/ATISS/#" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2110.03675.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2021NEURIPS_poster.pdf" data-type="Poster">Poster</a> <a href="data/Paschalidou2021NEURIPS_slides.pdf" data-type="Slides">Slides</a> <a href="https://github.com/nv-tlabs/atiss" data-type="Code">Code</a> <a href="#" data-type="Bibtex" data-index="6">Bibtex</a><div class="link-content" data-index="0">The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8x faster than existing methods.</div><div class="link-content" data-index="6"><pre>@inproceedings{Paschalidou2021NEURIPS,
  author = {Despoina Paschalidou and Amlan Kar and Maria Shugrina and Karsten Kreis and Andreas Geiger and Sanja Fidler},
  title = {ATISS: Autoregressive Transformers for Indoor Scene Synthesis},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/neural_parts.png" alt="Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks" /></div><div class="content"><div class="paper-title"><a href="https://paschalidoud.github.io/neural_parts">Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2021</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://angeloskath.github.io/" class="author">Angelos Katharopoulos</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://www.cs.utoronto.ca/~fidler/" class="author">Sanja Fidler</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://paschalidoud.github.io/neural_parts" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2103.10429.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2021CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/neural_parts" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/neural-parts/" data-type="Blog">Blog</a> <a href="http://www.cvlibs.net/publications/Paschalidou2021CVPR_slides.pdf" data-type="Slides">Slides</a> <a href="https://www.youtube.com/watch?v=6WK3B0IZJsw" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="8">Bibtex</a><div class="link-content" data-index="0">Impressive progress in 3D shape extraction led to representations that can capture object geometries with high fidelity. In parallel, primitive-based methods seek to represent objects as semantically consistent part arrangements. However, due to the simplicity of existing primitive representations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We address the trade-off between reconstruction quality and number of parts with Neural Parts, a novel 3D primitive representation that defines primitives using an Invertible Neural Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efficient computation of both the implicit surface function of a primitive and its mesh, without any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrangements without any part-level supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods.</div><div class="link-content" data-index="8"><pre>@inproceedings{Paschalidou2021CVPR,
    title = {Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks},
    author = {Paschalidou, Despoina and Katharopoulos, Angelos and Geiger, Andreas and Fidler, Sanja},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2021}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/hierarchical_primitives.png" alt="Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image" /></div><div class="content"><div class="paper-title"><a href="http:superquadrics.com/hierarchical_primitives">Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2020</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" class="author">Luc van Gool</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="http:superquadrics.com/hierarchical_primitives" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2004.01176.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2020CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/hierarchical_primitives" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/hierarchical-primitives/" data-type="Blog">Blog</a> <a href="http://www.cvlibs.net/publications/Paschalidou2020CVPR_slides.pdf" data-type="Slides">Slides</a> <a href="https://www.youtube.com/watch?v=QgD0NHbWVlU&vq=hd1080&autoplay=1" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="8">Bibtex</a><div class="link-content" data-index="0">Humans perceive the 3D world as a set of distinct objects that are characterized by various low-level (geometry, reflectance) and high-level (connectivity, adjacency, symmetry) properties. Recent methods based on convolutional neural networks (CNNs) demonstrated impressive progress in 3D reconstruction, even when using a single 2D image as input. However, the majority of these methods focuses on recovering the local 3D geometry of an object without considering its part-based decomposition or relations between parts. We address this challenging problem by proposing a novel formulation that allows to jointly recover the geometry of a 3D object as a set of primitives as well as their latent hierarchical structure without part-level supervision. Our model recovers the higher level structural decomposition of various objects in the form of a binary tree of primitives, where simple parts are represented with fewer primitives and more complex parts are modeled with more components. Our experiments on the ShapeNet and D-FAUST datasets demonstrate that considering the organization of parts indeed facilitates reasoning about 3D geometry.</div><div class="link-content" data-index="8"><pre>@inproceedings{Paschalidou2020CVPR,
    title = {Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image},
    author = {Paschalidou, Despoina and Luc van Gool and Geiger, Andreas},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2020},
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="teasers/superquadrics_revisited.png" alt="Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids" /></div><div class="content"><div class="paper-title"><a href="http:superquadrics.com">Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR), 2019</div><div class="authors"><strong class="author">Despoina Paschalidou</strong>, <a href="https://scholar.google.de/citations?user=fkqdDEEAAAAJ&hl=en" class="author">Ali Osman Ulusoy</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="http:superquadrics.com/learnable-superquadrics.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/1904.09970.pdf" data-type="Paper">Paper</a> <a href="data/Paschalidou2019CVPR_poster.pdf" data-type="Poster">Poster</a> <a href="https://github.com/paschalidoud/superquadric_parsing" data-type="Code">Code</a> <a href="https://autonomousvision.github.io/superquadrics-revisited/" data-type="Blog">Blog</a> <a href="https://www.youtube.com/watch?v=eaZHYOsv9Lw" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="7">Bibtex</a><div class="link-content" data-index="0">Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.</div><div class="link-content" data-index="7"><pre>@inproceedings{Paschalidou2019CVPR,
    title = {Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids},
    author = {Paschalidou, Despoina and Ulusoy, Ali Osman and Geiger, Andreas},
    booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    month = jun,
    year = {2019},
}</pre></div></div></div></div><!-- end publication list -->

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143288088-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-143288088-1');
        </script>
    </body>
</html>
